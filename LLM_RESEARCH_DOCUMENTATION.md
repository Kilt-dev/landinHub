# ğŸ”¬ TÃ i liá»‡u NghiÃªn cá»©u LLM - Landing Hub Chatbot

## Tá»•ng quan NghiÃªn cá»©u

ÄÃ¢y lÃ  **há»‡ thá»‘ng nghiÃªn cá»©u vÃ  Ä‘Ã¡nh giÃ¡ Large Language Models (LLMs)** Ä‘Æ°á»£c xÃ¢y dá»±ng cho chatbot há»— trá»£ Landing Hub. Má»¥c tiÃªu lÃ  so sÃ¡nh hiá»‡u nÄƒng, cháº¥t lÆ°á»£ng vÃ  chi phÃ­ cá»§a cÃ¡c mÃ´ hÃ¬nh AI khÃ¡c nhau trong bá»‘i cáº£nh thá»±c táº¿.

### ğŸ¯ Má»¥c tiÃªu NghiÃªn cá»©u

1. **So sÃ¡nh Performance**: ÄÃ¡nh giÃ¡ latency, throughput cá»§a cÃ¡c LLM providers
2. **ÄÃ¡nh giÃ¡ Quality**: Relevance, accuracy, coherence, helpfulness
3. **PhÃ¢n tÃ­ch Cost**: Chi phÃ­ sá»­ dá»¥ng thá»±c táº¿ trÃªn production
4. **A/B Testing**: XÃ¡c Ä‘á»‹nh model tá»‘i Æ°u cho tá»«ng use case
5. **Academic Contribution**: Cung cáº¥p data cho nghiÃªn cá»©u academic

---

## ğŸ§ª PhÆ°Æ¡ng phÃ¡p NghiÃªn cá»©u

### 1. Test Scenarios

Há»‡ thá»‘ng test 5 categories chÃ­nh:

#### **Marketplace Queries**
- Template recommendations
- Trend analysis
- Pricing comparisons
- Category analytics
- Sales insights

#### **Builder Instructions**
- Drag-drop tutorials
- Property editing guides
- Responsive design help
- Keyboard shortcuts
- Layer management

#### **Analytics Questions**
- User page statistics
- Conversion rate analysis
- Sales performance
- Form submissions tracking
- Competitor comparison

#### **Deployment Guidance**
- CloudFront deployment
- Custom domain setup
- SSL configuration
- DNS management
- Optimization tips

#### **General Support**
- Platform introduction
- Feature explanations
- Pricing information
- Payment methods
- Getting started guides

### 2. Evaluation Metrics

Má»—i response Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ theo **5 tiÃªu chÃ­ chÃ­nh**:

#### **A. Relevance Score (35% weight)**
- CÃ¢u tráº£ lá»i cÃ³ liÃªn quan Ä‘áº¿n cÃ¢u há»i khÃ´ng?
- CÃ³ Ä‘á» cáº­p Ä‘áº¿n keywords quan trá»ng?
- CÃ³ tráº£ lá»i Ä‘Ãºng váº¥n Ä‘á» user há»i?
- **Scale**: 1-10

#### **B. Accuracy Score (25% weight)**
- ThÃ´ng tin cÃ³ chÃ­nh xÃ¡c khÃ´ng?
- CÃ³ dá»¯ liá»‡u cá»¥ thá»ƒ (sá»‘ liá»‡u, giÃ¡, tÃªn...)?
- CÃ³ sai lá»‡ch vá» technical details?
- **Scale**: 1-10

#### **C. Coherence Score (20% weight)**
- CÃ¢u tráº£ lá»i cÃ³ cáº¥u trÃºc rÃµ rÃ ng?
- Logic cÃ³ máº¡ch láº¡c?
- Äá»™ dÃ i phÃ¹ há»£p (khÃ´ng quÃ¡ ngáº¯n/dÃ i)?
- CÃ³ sá»­ dá»¥ng formatting (bullets, numbers)?
- **Scale**: 1-10

#### **D. Helpfulness Score (20% weight)**
- CÃ³ actionable steps khÃ´ng?
- User cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c ngay?
- CÃ³ vÃ­ dá»¥ cá»¥ thá»ƒ?
- CÃ³ tips/suggestions há»¯u Ã­ch?
- **Scale**: 1-10

#### **E. Overall Score**
```
Overall = (Relevance Ã— 0.35) + (Accuracy Ã— 0.25) +
          (Coherence Ã— 0.20) + (Helpfulness Ã— 0.20)
```

### 3. Performance Metrics

#### **Latency**
- Time from request to complete response
- Measured in milliseconds
- **Target**: < 2000ms for good UX

#### **Tokens Used**
- Input tokens + Output tokens
- Estimated using char count / 4
- Important for cost calculation

#### **Estimated Cost**
```javascript
// OpenAI GPT-4o-mini
cost = (tokens / 1,000,000) Ã— $0.15  // Input
     + (tokens / 1,000,000) Ã— $0.60  // Output

// Groq, Gemini, Ollama
cost = $0.00  // FREE
```

---

## ğŸ“Š Benchmark System Architecture

### Database Schema

```javascript
LLMBenchmark {
  test_id: String,           // Unique test identifier
  test_name: String,         // Test description
  test_category: String,     // marketplace|builder|analytics|deployment|general
  prompt: String,            // User question

  responses: [{
    provider: String,        // groq|openai|gemini|ollama
    model: String,          // llama-3.3-70b|gpt-4o-mini|gemini-1.5-flash
    response: String,       // AI response text

    // Performance metrics
    latency_ms: Number,
    tokens_used: Number,
    estimated_cost: Number,
    timestamp: Date,

    // Quality scores (1-10)
    relevance_score: Number,
    accuracy_score: Number,
    coherence_score: Number,
    helpfulness_score: Number,
    overall_score: Number,

    // Evaluation metadata
    evaluated_by: String,   // 'auto' | userId
    evaluation_method: String // 'semantic' | 'manual' | 'user_feedback'
  }],

  winner: String,           // Provider with highest overall_score
  created_at: Date
}
```

### Automated Evaluation Algorithm

```javascript
// 1. Relevance Detection
- Check for topic keywords
- Vietnamese language quality
- Question-answer alignment
â†’ Score: 1-10

// 2. Accuracy Assessment
- Contains numbers/data?
- Specific information?
- Technical correctness?
â†’ Score: 1-10

// 3. Coherence Analysis
- Sentence count (2-6 ideal)
- Has structure (bullets/numbers)?
- Logical flow?
â†’ Score: 1-10

// 4. Helpfulness Evaluation
- Actionable steps?
- Click/action verbs?
- Detailed enough?
â†’ Score: 1-10

// 5. Overall Weighted Score
Overall = Î£(score_i Ã— weight_i)
```

---

## ğŸš€ API Endpoints

### 1. Run Benchmark

```http
POST /api/research/benchmark/run
Authorization: Bearer {admin_token}

Body:
{
  "providers": ["groq", "openai", "gemini"],
  "category": "marketplace",  // or "all"
  "userId": "optional_user_id"
}

Response:
{
  "success": true,
  "message": "Benchmark started in background",
  "providers": ["groq", "openai", "gemini"],
  "category": "marketplace",
  "estimatedTime": "15 - 30 minutes"
}
```

### 2. Get Statistics

```http
GET /api/research/benchmark/stats?category=marketplace
Authorization: Bearer {admin_token}

Response:
{
  "success": true,
  "stats": {
    "totalTests": 50,
    "byCategory": {
      "marketplace": 20,
      "builder": 15,
      "analytics": 15
    },
    "byProvider": {
      "groq": 50,
      "openai": 50,
      "gemini": 50
    },
    "avgScores": {
      "groq": "8.45",
      "openai": "8.72",
      "gemini": "8.21"
    },
    "avgLatency": {
      "groq": 1250,
      "openai": 1850,
      "gemini": 2100
    },
    "totalCost": {
      "groq": 0,
      "openai": 0.042,
      "gemini": 0
    },
    "winRate": {
      "groq": "38.0%",
      "openai": "44.0%",
      "gemini": "18.0%"
    }
  }
}
```

### 3. Compare Providers

```http
GET /api/research/benchmark/compare?provider1=groq&provider2=openai&category=all
Authorization: Bearer {admin_token}

Response:
{
  "success": true,
  "comparison": {
    "totalTests": 50,
    "category": "all",
    "comparison": {
      "groq": {
        "wins": 19,
        "avgScore": "8.45",
        "avgLatency": 1250,
        "totalCost": 0
      },
      "openai": {
        "wins": 22,
        "avgScore": "8.72",
        "avgLatency": 1850,
        "totalCost": 0.042
      }
    }
  }
}
```

### 4. Export Data (CSV)

```http
GET /api/research/benchmark/export?category=all
Authorization: Bearer {admin_token}

Response: CSV file download
```

---

## ğŸ“ˆ Káº¿t quáº£ NghiÃªn cá»©u Máº«u

### Performance Comparison (50 tests)

| Provider | Avg Score | Avg Latency | Win Rate | Total Cost |
|----------|-----------|-------------|----------|------------|
| **OpenAI GPT-4o-mini** | 8.72 | 1,850ms | 44% | $0.042 |
| **Groq Llama 3.3 70B** | 8.45 | 1,250ms | 38% | $0.00 |
| **Gemini 1.5 Flash** | 8.21 | 2,100ms | 18% | $0.00 |
| **Ollama Llama 3.2** | 7.89 | 3,200ms | 0% | $0.00 |

### Quality Breakdown

#### OpenAI GPT-4o-mini
- âœ… **Strengths**: Highest accuracy, best coherence
- âœ… Vietnamese natural, specific answers
- âŒ **Weaknesses**: Slower, costs money
- **Best for**: Production, high-quality responses

#### Groq Llama 3.3 70B
- âœ… **Strengths**: Fastest (1.25s avg), FREE, good quality
- âœ… Great for Vietnamese, handles context well
- âŒ **Weaknesses**: Slightly less accurate than GPT-4o
- **Best for**: Development, high-traffic, budget-conscious

#### Google Gemini 1.5 Flash
- âœ… **Strengths**: FREE, decent quality
- âœ… Good for general questions
- âŒ **Weaknesses**: Slower, less relevant for Vietnamese
- **Best for**: Backup, multi-lingual support

#### Ollama Llama 3.2 (Local)
- âœ… **Strengths**: 100% free, privacy, offline
- âŒ **Weaknesses**: Slowest, lowest quality, needs GPU
- **Best for**: Development, testing, no internet

---

## ğŸ”¬ PhÆ°Æ¡ng phÃ¡p Benchmark

### Automated Benchmark Script

```bash
# Run full benchmark (all categories, all providers)
curl -X POST http://localhost:5000/api/research/benchmark/run \
  -H "Authorization: Bearer YOUR_ADMIN_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "providers": ["groq", "openai", "gemini"],
    "category": "all"
  }'
```

### Test Prompts

Há»‡ thá»‘ng sá»­ dá»¥ng **25 test prompts** chuáº©n:

**Marketplace (5 prompts)**:
- "Template nÃ o Ä‘ang bÃ¡n cháº¡y nháº¥t?"
- "Xu hÆ°á»›ng marketplace hiá»‡n táº¡i nhÆ° tháº¿ nÃ o?"
- "So sÃ¡nh giÃ¡ template E-commerce vÃ  Business"
- "Gá»£i Ã½ template phÃ¹ há»£p cho startup"
- "PhÃ¢n tÃ­ch bestsellers theo category"

**Builder (5 prompts)**:
- "LÃ m sao Ä‘á»ƒ kÃ©o tháº£ element vÃ o canvas?"
- "CÃ¡ch chá»‰nh sá»­a properties cá»§a button?"
- "Thiáº¿t káº¿ responsive trÃªn mobile nhÆ° tháº¿ nÃ o?"
- "Keyboard shortcuts há»¯u Ã­ch trong builder?"
- "Quáº£n lÃ½ layers vÃ  z-index ra sao?"

*(...tiáº¿p tá»¥c cho cÃ¡c categories khÃ¡c)*

### Manual Evaluation

Admin cÃ³ thá»ƒ Ä‘Ã¡nh giÃ¡ manually qua UI:

1. Xem benchmark results
2. Rate tá»«ng response (1-10 cho má»—i metric)
3. Select winner
4. Add comments/feedback

---

## ğŸ“Š Data Analysis & Visualization

### Statistical Analysis

```javascript
// Aggregate statistics
const stats = {
  mean: Î£(scores) / n,
  median: scores[n/2],
  stdDev: âˆš(Î£(x - mean)Â² / n),
  variance: ÏƒÂ²,
  min: min(scores),
  max: max(scores)
}

// Win rate calculation
winRate = (wins / totalTests) Ã— 100%

// Cost efficiency
costPerResponse = totalCost / totalResponses
costPerQualityPoint = totalCost / Î£(overallScores)
```

### Visualization Recommendations

1. **Bar Chart**: Average scores by provider
2. **Line Chart**: Latency over time
3. **Scatter Plot**: Quality vs Latency
4. **Pie Chart**: Win rate distribution
5. **Heatmap**: Score breakdown by category

---

## ğŸ“ Academic Contributions

### Research Questions

1. **RQ1**: LÃ m tháº¿ nÃ o open-source LLMs (Llama 3.3) so vá»›i proprietary models (GPT-4) trong Vietnamese chatbot context?

2. **RQ2**: Trade-off giá»¯a latency, quality, vÃ  cost nhÆ° tháº¿ nÃ o khi deploy production chatbot?

3. **RQ3**: Context injection (real-time data) áº£nh hÆ°á»Ÿng Ä‘áº¿n response quality ra sao?

4. **RQ4**: Multi-provider fallback strategy hiá»‡u quáº£ nhÆ° tháº¿ nÃ o?

### Methodology

- **Quantitative**: Automated scoring (relevance, accuracy, coherence, helpfulness)
- **Qualitative**: Manual evaluation by domain experts
- **Mixed**: User feedback trong production
- **Longitudinal**: Tracking performance over time

### Data Collection

- **Sample Size**: 500+ benchmark tests
- **Duration**: 1 month continuous testing
- **Categories**: 5 major use cases
- **Providers**: 4 LLM providers
- **Evaluation**: Automated + Manual + User feedback

### Expected Findings

**H1**: Groq Llama 3.3 70B sáº½ cÃ³ latency tháº¥p nháº¥t nhÆ°ng quality tháº¥p hÆ¡n GPT-4o-mini 5-10%

**H2**: Context injection tÄƒng accuracy 15-25% so vá»›i generic responses

**H3**: Multi-provider fallback giáº£m downtime 90%+

**H4**: Cost optimization vá»›i Groq tiáº¿t kiá»‡m 100% chi phÃ­ so vá»›i OpenAI trong production

---

## ğŸ“ Publication-Ready Results

### Data Export Formats

1. **CSV**: Raw benchmark data
2. **JSON**: Structured results
3. **LaTeX Table**: For academic papers
4. **Markdown**: For documentation

### Citation Format

```bibtex
@article{landing_hub_llm_2025,
  title={Comparative Analysis of Large Language Models for Vietnamese E-commerce Chatbot},
  author={Your Name},
  journal={Conference/Journal Name},
  year={2025},
  note={Landing Hub LLM Benchmark Study}
}
```

---

## ğŸ”§ Implementation Details

### Groq Setup (RECOMMENDED)

```bash
# 1. Get API Key from https://console.groq.com/
# 2. Add to .env
GROQ_API_KEY=gsk_your_key_here
GROQ_MODEL=llama-3.3-70b-versatile
AI_PROVIDER=groq

# 3. Test
curl http://localhost:5000/api/chat/provider-status
```

### Running Benchmarks

```javascript
// Programmatic benchmark
const results = await runBenchmark(
  ['groq', 'openai', 'gemini'],  // providers
  'marketplace',                  // category
  userId                         // optional
);

console.log(`Completed ${results.length} tests`);
```

### Analyzing Results

```javascript
// Get comprehensive stats
const stats = await getBenchmarkStats({
  category: 'marketplace'
});

// Compare two providers
const comparison = await compareProviders(
  'groq',
  'openai',
  'all'
);

// Export to CSV
const csv = await exportBenchmarkCSV({
  category: 'all'
});
```

---

## ğŸ¯ Research Roadmap

### Phase 1: Initial Benchmark (Complete)
- [x] Setup multi-provider system
- [x] Implement automated evaluation
- [x] Create test prompts
- [x] Build API endpoints

### Phase 2: Data Collection (In Progress)
- [ ] Run 500+ benchmark tests
- [ ] Collect user feedback
- [ ] Manual quality evaluation
- [ ] Performance monitoring

### Phase 3: Analysis (Upcoming)
- [ ] Statistical analysis
- [ ] Visualization dashboards
- [ ] Academic paper draft
- [ ] Public dataset release

### Phase 4: Optimization (Future)
- [ ] Fine-tuning experiments
- [ ] Prompt engineering optimization
- [ ] Hybrid model strategies
- [ ] Cost-quality balancing

---

## ğŸ“š References & Resources

### Papers
- Attention Is All You Need (Vaswani et al., 2017)
- LLaMA: Open and Efficient Foundation Language Models (Touvron et al., 2023)
- GPT-4 Technical Report (OpenAI, 2023)

### Datasets
- Vietnamese NLP Datasets
- E-commerce Support Conversations
- Landing Hub Production Logs

### Tools
- Groq API: https://console.groq.com
- OpenAI API: https://platform.openai.com
- Google Gemini: https://ai.google.dev

---

## ğŸ’¡ Academic Tips

### TÄƒng tÃ­nh há»c thuáº­t

1. **Benchmark nhiá»u scenarios**: KhÃ´ng chá»‰ chat, test cáº£ summarization, classification, translation

2. **Fine-tuning experiments**: Thá»­ fine-tune Llama 3.2 trÃªn Landing Hub data

3. **Human evaluation**: Thu tháº­p feedback tá»« real users

4. **Error analysis**: PhÃ¢n tÃ­ch failure cases chi tiáº¿t

5. **Reproducibility**: Document má»i parameter, random seeds

6. **Ablation studies**: Test tá»«ng component riÃªng láº»

7. **Statistical significance**: T-tests, ANOVA cho comparisons

8. **Qualitative analysis**: Case studies, examples

### TrÃ¡nh háº¡n cháº¿

âŒ **KHÃ”NG**: Chá»‰ dÃ¹ng API nhÆ° ChatGPT
âœ… **NÃŠN**: Benchmark, compare, analyze, optimize

âŒ **KHÃ”NG**: Chá»‰ test 1-2 prompts
âœ… **NÃŠN**: Comprehensive test suite (25+ prompts)

âŒ **KHÃ”NG**: Subjective evaluation
âœ… **NÃŠN**: Automated metrics + manual validation

âŒ **KHÃ”NG**: Ignore cost/latency
âœ… **NÃŠN**: Multi-dimensional analysis (quality + performance + cost)

---

## ğŸ† Káº¿t luáº­n

Há»‡ thá»‘ng LLM Benchmark cá»§a Landing Hub cung cáº¥p:

1. **Framework nghiÃªn cá»©u** Ä‘áº§y Ä‘á»§ cho academic work
2. **Data-driven insights** cho production decisions
3. **Reproducible methodology** cho transparency
4. **Real-world application** khÃ´ng chá»‰ lÃ½ thuyáº¿t
5. **Cost optimization** strategies based on data

**Recommendation cuá»‘i**: Sá»­ dá»¥ng **Groq** cho production (fast + free + good quality), fallback to **OpenAI** khi cáº§n quality cao nháº¥t.

---

*Last updated: 2025*
*Landing Hub Research Team*
