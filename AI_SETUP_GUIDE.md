# ü§ñ H∆∞·ªõng d·∫´n Setup AI Provider cho Landing Hub Chatbot

## T·ªïng quan

Landing Hub Chatbot h·ªó tr·ª£ **4 AI providers**:
1. **OpenAI** (Paid - Production ready)
2. **Groq** (FREE - Fastest, Recommend!)
3. **Google Gemini** (FREE - Generous limits)
4. **Ollama** (FREE - Local, No internet needed)

System t·ª± ƒë·ªông ch·ªçn provider c√≥ s·∫µn theo th·ª© t·ª± ∆∞u ti√™n ho·∫∑c b·∫°n c√≥ th·ªÉ ch·ªâ ƒë·ªãnh.

---

## üî• Option 1: Groq (MI·ªÑN PH√ç - RECOMMEND)

### ∆Øu ƒëi·ªÉm:
- ‚úÖ **Ho√†n to√†n mi·ªÖn ph√≠**
- ‚úÖ **Si√™u nhanh** (nhanh h∆°n OpenAI 10-15x)
- ‚úÖ **30 requests/minute** (ƒë·ªß cho chatbot)
- ‚úÖ Models m·∫°nh: Llama 3.3 70B, Mixtral 8x7B

### Setup:

1. **ƒêƒÉng k√Ω t√†i kho·∫£n**:
   ```
   https://console.groq.com/
   ```

2. **L·∫•y API Key**:
   - Sau khi ƒëƒÉng nh·∫≠p ‚Üí **API Keys**
   - Click **Create API Key**
   - Copy key

3. **Th√™m v√†o `.env`**:
   ```bash
   # backend/.env
   GROQ_API_KEY=gsk_your_groq_api_key_here
   GROQ_MODEL=llama-3.3-70b-versatile  # ho·∫∑c mixtral-8x7b-32768
   AI_PROVIDER=groq
   ```

4. **Restart backend**:
   ```bash
   cd backend
   npm run dev
   ```

‚úÖ **Done!** Chat s·∫Ω d√πng Groq, si√™u nhanh v√† mi·ªÖn ph√≠!

---

## üíé Option 2: Google Gemini (MI·ªÑN PH√ç)

### ∆Øu ƒëi·ªÉm:
- ‚úÖ **Mi·ªÖn ph√≠** v·ªõi generous limits
- ‚úÖ **15 requests/minute**
- ‚úÖ **1 million tokens/day** FREE
- ‚úÖ Model: Gemini 1.5 Flash (r·∫•t nhanh)

### Setup:

1. **L·∫•y API Key**:
   ```
   https://ai.google.dev/
   ```
   - Click **Get API Key in Google AI Studio**
   - T·∫°o project m·ªõi ho·∫∑c ch·ªçn existing
   - Copy API key

2. **Th√™m v√†o `.env`**:
   ```bash
   # backend/.env
   GEMINI_API_KEY=AIzaSy...your_key_here
   GEMINI_MODEL=gemini-1.5-flash
   AI_PROVIDER=gemini
   ```

3. **Restart backend**

---

## üöÄ Option 3: OpenAI (PAID)

### ∆Øu ƒëi·ªÉm:
- ‚úÖ **$5 free credit** khi ƒëƒÉng k√Ω m·ªõi
- ‚úÖ **Production-ready**
- ‚úÖ GPT-4o-mini: ~$0.15/1M tokens (R·∫∫!)
- ‚úÖ Ch·∫•t l∆∞·ª£ng responses t·ªët nh·∫•t

### Gi√°:
- GPT-4o-mini: **$0.150** / 1M input tokens, **$0.600** / 1M output tokens
- ∆Ø·ªõc t√≠nh: **1,000 chat messages ‚âà $0.10-0.30** (r·∫•t r·∫ª)

### Setup:

1. **ƒêƒÉng k√Ω OpenAI**:
   ```
   https://platform.openai.com/signup
   ```

2. **Th√™m $5 credit** (ho·∫∑c d√πng free credit n·∫øu c√≥):
   - V√†o **Billing**
   - Add payment method
   - C√≥ $5 free credit cho t√†i kho·∫£n m·ªõi

3. **L·∫•y API Key**:
   ```
   https://platform.openai.com/api-keys
   ```
   - Click **Create new secret key**
   - Copy key (ch·ªâ hi·ªán 1 l·∫ßn!)

4. **Th√™m v√†o `.env`**:
   ```bash
   # backend/.env
   OPENAI_API_KEY=sk-proj-...your_key_here
   OPENAI_MODEL=gpt-4o-mini  # R·∫ª v√† nhanh
   AI_PROVIDER=openai
   ```

---

## üè† Option 4: Ollama (HO√ÄN TO√ÄN MI·ªÑN PH√ç - Local)

### ∆Øu ƒëi·ªÉm:
- ‚úÖ **100% mi·ªÖn ph√≠**
- ‚úÖ **Ch·∫°y local** - kh√¥ng c·∫ßn internet
- ‚úÖ **Kh√¥ng gi·ªõi h·∫°n** requests
- ‚úÖ **Privacy** - data kh√¥ng r·ªùi m√°y

### Nh∆∞·ª£c ƒëi·ªÉm:
- ‚ùå C·∫ßn GPU/RAM m·∫°nh (8GB+ RAM recommend)
- ‚ùå Ch·∫≠m h∆°n cloud APIs
- ‚ùå Quality th·∫•p h∆°n GPT-4

### Setup:

1. **Install Ollama**:

   **Mac:**
   ```bash
   brew install ollama
   ```

   **Linux:**
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

   **Windows:**
   Download t·ª´: https://ollama.ai/download

2. **Download model**:
   ```bash
   # Llama 3.2 (Recommend - 2GB)
   ollama pull llama3.2

   # Ho·∫∑c Mistral (4GB)
   ollama pull mistral

   # Ho·∫∑c Phi-3 (nh·∫π - 1.6GB)
   ollama pull phi3
   ```

3. **Start Ollama server**:
   ```bash
   ollama serve
   ```
   (M·∫∑c ƒë·ªãnh ch·∫°y port 11434)

4. **Th√™m v√†o `.env`**:
   ```bash
   # backend/.env
   OLLAMA_ENABLED=true
   OLLAMA_MODEL=llama3.2
   OLLAMA_URL=http://localhost:11434/api/chat
   AI_PROVIDER=ollama
   ```

5. **Restart backend**

---

## üéØ So s√°nh Providers

| Provider | Cost | Speed | Quality | Limits |
|----------|------|-------|---------|--------|
| **Groq** | FREE | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 30 req/min |
| **Gemini** | FREE | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 15 req/min, 1M tokens/day |
| **OpenAI** | $0.15-0.60/1M | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Unlimited (paid) |
| **Ollama** | FREE | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Unlimited (local) |

---

## üîß Configuration Options

### Auto Provider Selection

ƒê·ªÉ system t·ª± ch·ªçn provider c√≥ s·∫µn:
```bash
# backend/.env
AI_PROVIDER=auto  # Default
```

System s·∫Ω th·ª≠ theo th·ª© t·ª±: **Groq ‚Üí Gemini ‚Üí OpenAI ‚Üí Ollama**

### Force Specific Provider

```bash
AI_PROVIDER=groq     # Force Groq
AI_PROVIDER=gemini   # Force Gemini
AI_PROVIDER=openai   # Force OpenAI
AI_PROVIDER=ollama   # Force Ollama
```

### Fallback System

N·∫øu provider ch√≠nh l·ªói, system t·ª± ƒë·ªông fallback sang provider kh√°c c√≥ s·∫µn.

---

## üìù Example `.env` Configurations

### Config 1: Groq Primary (Recommend)
```bash
# Groq - FREE & Fast
GROQ_API_KEY=gsk_your_key
GROQ_MODEL=llama-3.3-70b-versatile
AI_PROVIDER=groq

# Fallback to OpenAI if Groq fails
OPENAI_API_KEY=sk-your_key
OPENAI_MODEL=gpt-4o-mini
```

### Config 2: Gemini Primary
```bash
# Gemini - FREE
GEMINI_API_KEY=AIzaSy_your_key
GEMINI_MODEL=gemini-1.5-flash
AI_PROVIDER=gemini
```

### Config 3: OpenAI Primary (Production)
```bash
# OpenAI - Best Quality
OPENAI_API_KEY=sk-your_key
OPENAI_MODEL=gpt-4o-mini
AI_PROVIDER=openai
```

### Config 4: Ollama Local
```bash
# Ollama - 100% Free & Local
OLLAMA_ENABLED=true
OLLAMA_MODEL=llama3.2
OLLAMA_URL=http://localhost:11434/api/chat
AI_PROVIDER=ollama
```

---

## üß™ Testing

Sau khi setup, test chatbot:

1. **Start backend**:
   ```bash
   cd backend
   npm run dev
   ```

2. **Start frontend**:
   ```bash
   cd apps/web
   npm start
   ```

3. **M·ªü chat** v√† th·ª≠:
   - "Template n√†o ph·ªï bi·∫øn?"
   - "Page c·ªßa t√¥i c√≥ bao nhi√™u views?"
   - "L√†m sao k√©o th·∫£ element?"

4. **Check logs** - Backend s·∫Ω log provider ƒëang d√πng:
   ```
   ü§ñ Using AI provider: Groq (llama-3.3-70b-versatile)
   ```

---

## üí° Recommendations

### Cho Development:
‚úÖ **Groq** - Mi·ªÖn ph√≠, nhanh, ƒë·ªß x√†i

### Cho Production (Traffic th·∫•p):
‚úÖ **Groq** ho·∫∑c **Gemini** - Free tier ƒë·ªß

### Cho Production (Traffic cao):
‚úÖ **OpenAI** GPT-4o-mini - Gi√° r·∫ª ($0.15/1M), ch·∫•t l∆∞·ª£ng t·ªët

### Cho Offline/Privacy:
‚úÖ **Ollama** - Local, kh√¥ng c·∫ßn internet

---

## üîç Monitoring Provider Status

API endpoint ƒë·ªÉ check provider:

```bash
GET /api/chat/provider-status
```

Response:
```json
{
  "providers": [
    {
      "provider": "groq",
      "name": "Groq",
      "enabled": true,
      "model": "llama-3.3-70b-versatile",
      "active": true
    },
    {
      "provider": "openai",
      "name": "OpenAI",
      "enabled": true,
      "model": "gpt-4o-mini",
      "active": false
    }
  ]
}
```

---

## üÜò Troubleshooting

### "No AI provider configured" error

**Solution**: Th√™m √≠t nh·∫•t 1 provider v√†o `.env`:
```bash
GROQ_API_KEY=your_key
# ho·∫∑c
GEMINI_API_KEY=your_key
# ho·∫∑c
OPENAI_API_KEY=your_key
# ho·∫∑c
OLLAMA_ENABLED=true
```

### Groq/Gemini rate limit

**Solution**: System t·ª± fallback sang provider kh√°c, ho·∫∑c add th√™m providers

### Ollama kh√¥ng connect

**Solution**:
```bash
# Check Ollama running
ollama list

# Restart Ollama
ollama serve

# Test
curl http://localhost:11434/api/tags
```

---

## üí∞ Cost Estimation (OpenAI)

### Typical Chat Usage:
- 1 user message: ~100 tokens
- 1 AI response: ~300 tokens
- **1 chat exchange**: ~400 tokens

### Monthly Cost:
- **100 chats/day** = 3,000 chats/month
- 3,000 √ó 400 tokens = **1.2M tokens**
- Cost: **$0.18 - $0.72/month** (GPT-4o-mini)

**K·∫øt lu·∫≠n**: C·ª±c k·ª≥ r·∫ª! Chatbot Landing Hub c√≥ th·ªÉ d√πng OpenAI v·ªõi budget < $1/month

---

## üéâ Conclusion

**Recommendation cu·ªëi c√πng**:

1. **Start v·ªõi Groq (FREE)** - Setup 5 ph√∫t, mi·ªÖn ph√≠, nhanh
2. **N·∫øu c·∫ßn quality h∆°n** ‚Üí OpenAI GPT-4o-mini ($0.15/1M)
3. **N·∫øu c·∫ßn privacy** ‚Üí Ollama local

Happy chatting! üöÄ
